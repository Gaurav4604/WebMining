{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "virtualenv",
   "display_name": "virtualEnv",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the files\n",
    "chess = open('chess.txt','r').read()\n",
    "cricket = open('cricket.txt','r').read()\n",
    "tennis = open('tennis.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of common words are 52\nNumber of unique terms in chess.txt is 634\nNumber of unique terms in cricket.txt is 592\nNumber of unique terms in tennis.txt is 483\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words_chess = set(word_tokenize(chess))\n",
    "words_cricket = set(word_tokenize(cricket))\n",
    "words_tennis = set(word_tokenize(tennis))\n",
    "\n",
    "common = words_chess & words_cricket & words_tennis\n",
    "\n",
    "unique_chess = words_chess.difference(common)\n",
    "unique_cricket = words_cricket.difference(common)\n",
    "unique_tennis = words_tennis.difference(common)\n",
    "\n",
    "print(\"Number of common words are {}\".format(len(common)))\n",
    "print(\"Number of unique terms in chess.txt is {}\".format(len(unique_chess)))\n",
    "print(\"Number of unique terms in cricket.txt is {}\".format(len(unique_cricket)))\n",
    "print(\"Number of unique terms in tennis.txt is {}\".format(len(unique_tennis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of terms common after removal of stop words = 9\nNumber of terms unique in chess.txt after removal of stop words = 590\nNumber of terms unique in cricket.txt after removal of stop words = 523\nNumber of terms unique in tennis.txt after removal of stop words = 425\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stop_words = set(STOP_WORDS)\n",
    "stop_words = stop_words.union({'.',',','\\'','\\\"','?','{','}','[',']','<','>','(',')','!'})\n",
    "\n",
    "common_stop = common.difference(stop_words)\n",
    "chess_stop = unique_chess.difference(stop_words)\n",
    "cricket_stop = unique_cricket.difference(stop_words)\n",
    "tennis_stop = unique_tennis.difference(stop_words)\n",
    "print(\"Number of terms common after removal of stop words = {}\".format(len(common_stop)))\n",
    "print(\"Number of terms unique in chess.txt after removal of stop words = {}\".format(len(chess_stop)))\n",
    "print(\"Number of terms unique in cricket.txt after removal of stop words = {}\".format(len(cricket_stop)))\n",
    "print(\"Number of terms unique in tennis.txt after removal of stop words = {}\".format(len(tennis_stop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    values = [common_stop,chess_stop,cricket_stop,tennis_stop]\n",
    "    with open(\"index.txt\", \"w\") as output:\n",
    "        for row in values:\n",
    "                s = \" \".join(map(str, row))\n",
    "                output.write(s+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of stemmed terms is 1240\nNumber of lemmatized terms is 1397\n"
     ]
    }
   ],
   "source": [
    "index = list(set(word_tokenize(open('index.txt','r').read())))\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "stemmed = list(set([stemmer.stem(w) for w in index]))\n",
    "lemmatized = list(set([lemmatizer.lemmatize(w) for w in index]))\n",
    "\n",
    "print(\"Number of stemmed terms is {}\".format(len(stemmed)))\n",
    "print(\"Number of lemmatized terms is {}\".format(len(lemmatized)))\n",
    "\n",
    "final_index = [stemmed,lemmatized]\n",
    "if len(stemmed)<len(lemmatized):\n",
    "    with open(\"index.txt\", \"w\") as output:\n",
    "        s = \" \".join(map(str, stemmed))\n",
    "        output.write(s+'\\n')\n",
    "else:\n",
    "    with open(\"index.txt\", \"w\") as output:\n",
    "        s = \" \".join(map(str, lemmatized))\n",
    "        output.write(s+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.rename('index.txt','index-final.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           Terms           POS-Tag\n",
       "0        dunedin     (dunedin, NN)\n",
       "1          light       (light, NN)\n",
       "2          singl       (singl, NN)\n",
       "3           2017        (2017, CD)\n",
       "4          allur       (allur, NN)\n",
       "...          ...               ...\n",
       "1235      dramat      (dramat, NN)\n",
       "1236        issu        (issu, NN)\n",
       "1237       admit       (admit, NN)\n",
       "1238        plan        (plan, NN)\n",
       "1239  tournament  (tournament, NN)\n",
       "\n",
       "[1240 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Terms</th>\n      <th>POS-Tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dunedin</td>\n      <td>(dunedin, NN)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>light</td>\n      <td>(light, NN)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>singl</td>\n      <td>(singl, NN)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017</td>\n      <td>(2017, CD)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>allur</td>\n      <td>(allur, NN)</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1235</th>\n      <td>dramat</td>\n      <td>(dramat, NN)</td>\n    </tr>\n    <tr>\n      <th>1236</th>\n      <td>issu</td>\n      <td>(issu, NN)</td>\n    </tr>\n    <tr>\n      <th>1237</th>\n      <td>admit</td>\n      <td>(admit, NN)</td>\n    </tr>\n    <tr>\n      <th>1238</th>\n      <td>plan</td>\n      <td>(plan, NN)</td>\n    </tr>\n    <tr>\n      <th>1239</th>\n      <td>tournament</td>\n      <td>(tournament, NN)</td>\n    </tr>\n  </tbody>\n</table>\n<p>1240 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import nltk\n",
    "untagged = list(set(word_tokenize(open('index-final.txt','r').read())))\n",
    "tagged = nltk.pos_tag(untagged)\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame({\"Terms\":untagged,\"POS-Tag\":tagged})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}