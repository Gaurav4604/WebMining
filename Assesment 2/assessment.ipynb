{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "virtualenv",
   "display_name": "virtualEnv",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfs concurrency\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "import re\n",
    "import threading\n",
    "\n",
    "def dfs(url):\n",
    "    visited = set([url])\n",
    "    dq = deque([[url, \"\", 0]])\n",
    "    max_depth = 3\n",
    "    max_breadth = 5\n",
    "    string = \"\"\n",
    "    while dq:\n",
    "        base, path, depth = dq.popleft()\n",
    "        if depth < max_depth:\n",
    "            try:\n",
    "                soup = BeautifulSoup(requests.get(base + path).text, \"html.parser\")\n",
    "                links = soup.find_all(\"a\")[:max_breadth]\n",
    "                for link in links:\n",
    "                    href = link.get(\"href\")\n",
    "                    if href not in visited:\n",
    "                        visited.add(href)\n",
    "                        string += (\"  \" * depth + f\"at depth {depth}: {href}\\n\")\n",
    "\n",
    "                        if href.startswith(\"http\") or href.startswith(\"https\"):\n",
    "                            dq.append([href, \"\", depth + 1])\n",
    "                        else:\n",
    "                            dq.append([base, href, depth + 1])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    filename =\"\"\n",
    "    if \"https://\" in url:\n",
    "        filename = re.sub(r\"https://\",\"\",url)\n",
    "        filename = re.sub(r\".com\",\"\",filename)\n",
    "    elif \"http://\" in url:\n",
    "        filename = re.sub(r\"http://\",\"\",url)\n",
    "        filename = re.sub(r\".com\",\"\",filename)\n",
    "\n",
    "    file  = open(\"crawled_links/\"+filename+\".txt\",\"w\")\n",
    "    file.write(string)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "files made!\n"
     ]
    }
   ],
   "source": [
    "frontier = [\"http://toscrape.com\", \"https://soundcloud.com\", \"http://reddit.com\", \"https://fc2.com\"]\n",
    "\n",
    "def threadCreater(frontier):\n",
    "    threads = []\n",
    "    for i in frontier:\n",
    "        t = threading.Thread(target = dfs, args=(i,))\n",
    "        threads.append(t)\n",
    "    return threads\n",
    "\n",
    "threads = threadCreater(frontier)\n",
    "for i in threads:\n",
    "    i.start()\n",
    "for i in threads:\n",
    "    i.join()\n",
    "print(\"files made!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "000011010\n000011011\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def unary(x):\n",
    "    return '0'*(x-1)+'1'\n",
    "\n",
    "def encoder(x, b):\n",
    "    q = math.floor(x/b)\n",
    "    q_encode = unary(q+1)\n",
    "    i = math.floor(math.log2(b))\n",
    "    d = int(2**(i+1))-b\n",
    "    r = x%b\n",
    "    rem = \"\"\n",
    "    if r < d:\n",
    "        rem = bin(r)[2:]\n",
    "        l = len(rem)\n",
    "        if l < i:\n",
    "            rem = '0'*(i - l)+rem\n",
    "    else:\n",
    "        rem = bin(r + d)[2:]\n",
    "        l = len(rem)\n",
    "        if l < i+1:\n",
    "            rem = '0'*(i+1-l) + rem\n",
    "    return q_encode+rem\n",
    "\n",
    "print(encoder(74, 16))\n",
    "print(encoder(50, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "def decode(n, M):\n",
    "    q=len(n.split('1')[0])\n",
    "    b=math.floor(math.log2(M))\n",
    "    k = 2**(b+1)-M\n",
    "    r = int(n[q+1:q+1+b],2)\n",
    "    if r>=k:\n",
    "        r = int(n[q+1:q+1+b+1],2)\n",
    "        r=r-k\n",
    "    x=q*M+r\n",
    "    return x\n",
    "\n",
    "print(decode(\"1111111110010001101\", 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from functools import reduce\n",
    "\n",
    "stop_words = set(STOP_WORDS)\n",
    "stop_words = stop_words.union({'.',',','\\'','\\\"','?','{','}','[',']','<','>','(',')','!'})\n",
    "\n",
    "def extract(url):\n",
    "    html = requests.urlopen(url).read().decode('utf8')\n",
    "    string = BeautifulSoup(html, 'html.parser').get_text()\n",
    "    return list(word_tokenize(string))\n",
    "\n",
    "def term_maker(ls, filenames):\n",
    "    terms = []\n",
    "    index = 0\n",
    "    for i in ls:\n",
    "        st = set(i).difference(stop_words)\n",
    "        file = open('extracted_content/'+filenames[i]+'.txt','w')\n",
    "        file.write(str(list(st)))\n",
    "        file.close()\n",
    "        index += 1\n",
    "        terms.append(list(st))\n",
    "    \n",
    "    return terms\n",
    "\n",
    "def counter(tokens, terms):\n",
    "    ls = []\n",
    "    for i in terms:\n",
    "        ls.append(tokens.count(i))\n",
    "    return ls"
   ]
  }
 ]
}